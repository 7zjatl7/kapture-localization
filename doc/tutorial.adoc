= kapture-localization / tutorial
:sectnums:
:sectnumlevels: 0
:toc:
:toclevels: 2

In this tutorial, you will learn how to localize query images within a map.
You will first see how to build the map using structure-from-motion using known poses.
Then, you will localize query images and evaluate the precision of the obtained localization against the ground truth.

We use the `virtual_gallery_tutorial` dataset as an example. It is a subset of the https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/[virtual gallery dataset].
You can find it in the `samples/` folder.
You can easily reproduce the procedure for any dataset that follows the <<recommended dataset structure>>.

If you want to use the pipelines to your own data, have a look to link:../pipeline/README.adoc[pipeline/README].

== recommended dataset structure

The pipeline scripts uses shared keypoints / descriptors / global_features / matches for multiple kapture folder.
To achieve that, they have to be stored outside any kapture folder and the pipeline scripts will be in charge of
rebuilding the correct inputs using symlinks.

[source,txt]
----
my_dataset
├─ mapping
│  └─ sensors
│     ├─ sensors.txt          # list of all sensors with their specifications (e.g. camera intrinsics)
│     ├─ trajectories.txt     # extrinsics (timestamp, sensor, pose)
│     ├─ records_camera.txt   # all records of type 'camera' (timestamp, sensor and path to image)
│     └─ records_data/        # image data path
├─ query
│  └─ sensors
│     ├─ sensors.txt          # list of all sensors with their specifications (e.g. camera intrinsics)
│     ├─ trajectories.txt     # extrinsics (timestamp, sensor, pose)
│     ├─ records_camera.txt   # all records of type 'camera' (timestamp, sensor and path to image)
│     └─ records_data/        # image data path
├─ map_plus_query # kapture_merge.py with mapping/query inputs
│  └─ sensors
│     ├─ sensors.txt          # list of all sensors with their specifications (e.g. camera intrinsics)
│     ├─ trajectories.txt     # extrinsics (timestamp, sensor, pose)
│     ├─ records_camera.txt   # all records of type 'camera' (timestamp, sensor and path to image)
│     └─ records_data/        # image data path
├─ local_features
│  ├─ r2d2_WASF-N8_20k
│  │  ├─ keypoints/
│  │  ├─ descriptors/
│  │  ├─ NN_no_gv # match method
│  │  │  └─ matches/ # non verified matches
│  │  └─ NN_colmap_gv/
│  │     └─ matches/  # verified matches
│  └─ d2_tf
│     ├─ keypoints/
│     ├─ descriptors/
│     ├─ NN_no_gv # match method
│     │  └─ matches/ # non verified matches
│     └─ NN_colmap_gv/
│        └─ matches/  # verified matches
└─ global_features
   └─ AP-GeM-LM18
      └─ global_features
----

WARNING: Windows users, you should have the file extension `.py` associated to the python 3.6 executable special rights
to allow symlink. see link:installation.adoc[installation instructions] for more details.

== install kapture-localization

see link:installation.adoc[]

For __Windows__ users, you must use `colmap.bat`. If the __colmap__ path is not available from your `%PATH%`
environment variable, you must provide it to kapture tools through the parameter `-colmap`,
e.g. `-colmap C:/Workspace/dev/colmap/colmap.bat`.


.running docker
[source,bash]
----
docker run --runtime=nvidia -it --rm  kapture/kapture-localization
cd kapture-localization
----

== preparing data

Before going through the <<kapture pipeline>>, you have to extract __local features__ and __global features__
for each image.

.precomputed features
For sake of simplicity, we provide precomputed local and global features for this tutorial  (virtual_gallery_tutorial).
The local features are __R2D2__ feature (500 keypoints per image).
They are stored in `./local_features/r2d2_500/{descriptors,keypoints}`.
The global features are __AP-GeM__ features.
They are stored in `global_features/AP-GeM-LM18/global_features/`.
If you want more details about those features or extract your owns, refer to the section below.

.extract your __local features__
You can extract __R2D2 local features__ using
https://github.com/naver/r2d2/blob/master/extract_kapture.py[extract_kapture.py] provided
in the https://github.com/naver/r2d2#feature-extraction-with-kapture-datasets[R2D2 git repository].
If you want to use another type of keypoints detector/descriptor, you just have to comply to the __kapture__ format.

.extract your __global features__
To extract __AP-GeM__ global features, use
the https://github.com/naver/deep-image-retrieval/blob/master/dirtorch/extract_kapture.py[extract_kapture.py] provided
in the https://github.com/naver/deep-image-retrieval#feature-extraction-with-kapture-datasets[deep-image-retrieval git repository]
If you want to use another type of global features, you just have to comply to the __kapture__ format.

You should put the mapping and query features in the same folder, see the <<recommended dataset structure>> above.

.previous experiments
Also, make sure, you start the tutorial cleaned from unwanted files (eg. previous experiments).

[source,bash]
----
cd samples/virtual_gallery_tutorial
./reset_tutorial_folder.py
----

== kapture pipeline

=== 1. mapping

[source,bash]
----
cd samples/virtual_gallery_tutorial # or your own dataset
# if the colmap executable is not available from your PATH,
# set the parameter -colmap. example -colmap C:/Workspace/dev/colmap/colmap.bat
kapture_pipeline_mapping.py -v info \
    -i ./mapping \
    -kpt ./local_features/r2d2_500/keypoints \
    -desc ./local_features/r2d2_500/descriptors \
    -gfeat ./global_features/AP-GeM-LM18/global_features \
    -matches ./local_features/r2d2_500/NN_no_gv/matches \
    -matches-gv ./local_features/r2d2_500/NN_colmap_gv/matches \
    --colmap-map ./colmap-sfm/r2d2_500/AP-GeM-LM18_top5  `# lfeat type / map pairs` \
    --topk 5
----

It will run in sequence:

 . `kapture_compute_image_pairs.py`: associate similar images within the mapping set,
 . `kapture_compute_matches.py`: compute 2D-2D matches using local features and the list of pairs,
 . `kapture_run_colmap_gv.py`: Run geometric verification on the 2D-2D matches,
 . `kapture_colmap_build_map.py` triangulate the 2D-2D matches to get 3D points and 2D-3D observations.

You will find the list of image pairs and the reconstruction inside `./colmap-sfm/r2d2_500/AP-GeM-LM18_top5`

To visualise the map, you can use __colmap__ gui, as follows:

[source,bash]
----
colmap gui \
    --database_path ./colmap-sfm/r2d2_500/AP-GeM-LM18_top5/colmap.db \
    --image_path ./mapping/sensors/records_data \
    --import_path ./colmap-sfm/r2d2_500/AP-GeM-LM18_top5/reconstruction/ # only available in colmap 3.6
----

NOTE: For Windows user, replace "colmap" with the full path to "colmap.bat",
as described in <<install kapture-localization>>.

NOTE: If you use an older version, you will have to import the model manually, on menu `file` > `import model` and browse to
`colmap-sfm/r2d2_500/AP-GeM-LM18_top5/reconstruction`. Click `yes` and `save` to the following dialogs.

As show in Fig. <<fig_reconstruct>>, the 3-D interface of __COLMAP__
shows the 3-D points and the cameras in the scene.
If you double-click on a camera, you'll see the image, and the 3-D points seen from it will be highlighted.

NOTE: If you are using docker, you can simply use __colmap__ GUI from host, even if the version is < 3.6.

.map reconstruction in __colmap__.
[[fig_reconstruct]]
image::../doc/colmap_mapping.jpg[reconstruction]


=== 2. localization

[source,bash]
----
# If the colmap executable is not available from your PATH, set the parameter -colmap
#   example: -colmap C:/Workspace/dev/colmap/colmap.bat
# If you are working with RobotCar or RobotCar_v2, add --prepend_cam
kapture_pipeline_localize.py -v info \
      -i ./mapping \
      --query ./query \
      -kpt ./local_features/r2d2_500/keypoints \
      -desc ./local_features/r2d2_500/descriptors \
      -gfeat ./global_features/AP-GeM-LM18/global_features \
      -matches ./local_features/r2d2_500/NN_no_gv/matches \
      -matches-gv ./local_features/r2d2_500/NN_colmap_gv/matches \
      --colmap-map ./colmap-sfm/r2d2_500/AP-GeM-LM18_top5 \
      -o ./colmap-localization/r2d2_500/AP-GeM-LM18_top5/AP-GeM-LM18_top5/ \
      --topk 5 \
      --config 2
----

It will run in sequence:

 . `kapture_compute_image_pairs.py` associates similar images between the mapping and query sets,
 . `kapture_merge.py` merges the mapping and query sensors into the same folder (necessary to compute matches),
 . `kapture_compute_matches.py` computes 2D-2D matches using local features and the list of pairs,
 . `kapture_run_colmap_gv.py` runs geometric verification on the 2D-2D matches,
 . `kapture_colmap_localize.py` runs the camera pose estimation part of the code,
 . `kapture_import_colmap.py` imports the colmap results into kapture,
 . `kapture_evaluate.py` If query ground truth is available, evaluates the localization.
 . `kapture_export_LTVL2020.py` exports the localized images to a format compatible with the
                                https://www.visuallocalization.net/ benchmark

In this script, the --config option will decide the parameters passed to colmap image_registrator.
The parameters are described in link:../kapture_localization/colmap/colmap_command.py[colmap_command.py]

In `./colmap-localization/r2d2_500/AP-GeM-LM18_top5/AP-GeM-LM18_top5/eval/stats.txt`,
you will find something similar to:

[source,ini]
----
Model: colmap_config_2

Found 4 / 4 image positions (100.00 %).
Found 4 / 4 image rotations (100.00 %).
Localized images: mean=(0.0124m, 0.2086 deg) / median=(0.0110m, 0.1675 deg)
All: median=(0.0110m, 0.1675 deg)
Min: 0.0030m; 0.0539 deg
Max: 0.0246m; 0.4454 deg

(0.25m, 2.0 deg): 100.00%
(0.5m, 5.0 deg): 100.00%
(5.0m, 10.0 deg): 100.00%
----

In `./colmap-localization/r2d2_500/AP-GeM-LM18_top5/AP-GeM-LM18_top5/LTVL2020_style_result.txt`,
you would have results compatible with https://www.visuallocalization.net/
if your dataset is part of this benchmark (not the case with virtual gallery).

To visualise the queries in the map, you can use __COLMAP__ gui, as follows:
[source,bash]
----
colmap gui \
    --database_path ./colmap-localization/r2d2_500/AP-GeM-LM18_top5/AP-GeM-LM18_top5/colmap_localized/colmap.db \
    --image_path query/sensors/records_data \
    --import_path ./colmap-localization/r2d2_500/AP-GeM-LM18_top5/AP-GeM-LM18_top5/colmap_localized/reconstruction/ # only available in colmap 3.6
----

.query localized in __colmap__.
[[fig_localized]]
image::../doc/colmap_localized.jpg[localized]

== Standard COLMAP pipeline

In contrast to <<kapture pipeline>>, this section walks through a simpler pipeline based SIFT local features
and Vocabulary Tree matching.

Make sure, you start the tutorial cleaned from unwanted files (eg. previous experiments).
see <<preparing data>>.

Then, download a vocabulary tree file from https://demuc.de/colmap/.
In this tutorial, we will use `vocab_tree_flickr100K_words32K.bin`.

[source,bash]
----
# Windows 10 includes curl.exe
curl -C - --output ./vocab_tree_flickr100K_words32K.bin --url https://demuc.de/colmap/vocab_tree_flickr100K_words32K.bin
----

[source,bash]
----
# if the colmap executable is not available from your PATH,
# set the parameter -colmap. example -colmap C:/Workspace/dev/colmap/colmap.bat
# If you are working with RobotCar or RobotCar_v2, add --prepend_cam
kapture_pipeline_colmap_vocab_tree.py -v info \
        -i ./mapping \
        --query ./query \
        -o ./sift_colmap_vocab_tree/ \
        -voc ./vocab_tree_flickr100K_words32K.bin \
        --config 2
----

It will run in sequence:

 . `kapture_colmap_build_sift_map.py` extracts sift features, run vocab tree matching, and point_triangulator
 . `kapture_colmap_localize_sift.py` extracts sift features, run vocab tree matching, and image_registrator
 . `kapture_import_colmap.py` imports the colmap results into kapture
 . `kapture_evaluate.py` If query ground truth is available, evaluates
 . `kapture_export_LTVL2020.py` exports the localized images to a format compatible with the
                                https://www.visuallocalization.net/ benchmark.

In this script, the --config option will decide the parameters passed to colmap image_registrator.
The parameters are described in link:../kapture_localization/colmap/colmap_command.py[colmap_command.py]

In `./sift_colmap_vocab_tree/eval/stats.txt`, you will find something similar to:
[source,bash]
----
Model: sift_colmap_vocab_tree_config_2

Found 4 / 4 image positions (100.00 %).
Found 4 / 4 image rotations (100.00 %).
Localized images: mean=(0.0027m, 0.0406 deg) / median=(0.0023m, 0.0407 deg)
All: median=(0.0023m, 0.0407 deg)
Min: 0.0020m; 0.0314 deg
Max: 0.0040m; 0.0495 deg

(0.25m, 2.0 deg): 100.00%
(0.5m, 5.0 deg): 100.00%
(5.0m, 10.0 deg): 100.00%
----

In `./sift_colmap_vocab_tree/LTVL2020_style_result.txt`, you would have results compatible with
https://www.visuallocalization.net/ if your dataset is part of this benchmark (not the case with virtual gallery).


